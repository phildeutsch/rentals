{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'rental_utils' from 'rental_utils.pyc'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from scipy.stats import randint as sp_randint\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "import rental_utils; reload(rental_utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_raw = rental_utils.clean(pd.read_json('Data/train.json'))\n",
    "test_raw = rental_utils.clean(pd.read_json('Data/test.json'))\n",
    "\n",
    "[features, feature_names] = rental_utils.get_features(25, train_raw)\n",
    "\n",
    "train = rental_utils.add_features(train_raw, features, feature_names)\n",
    "test = rental_utils.add_features(test_raw, features, feature_names)\n",
    "\n",
    "train = rental_utils.add_region(train)\n",
    "test = rental_utils.add_region(test)\n",
    "\n",
    "train = rental_utils.add_variables(train, train)\n",
    "test = rental_utils.add_variables(test, train)\n",
    "\n",
    "dv_county = rental_utils.vectorizer('County', train)\n",
    "train = rental_utils.one_hot_encode(dv_county, train, 'County')\n",
    "test = rental_utils.one_hot_encode(dv_county, test, 'County')\n",
    "\n",
    "dv_name = rental_utils.vectorizer('Name', train)\n",
    "train = rental_utils.one_hot_encode(dv_name, train, 'Name')\n",
    "test = rental_utils.one_hot_encode(dv_name, test, 'Name')\n",
    "\n",
    "dv_region = rental_utils.vectorizer('RegionID', train)\n",
    "train = rental_utils.one_hot_encode(dv_region, train, 'RegionID')\n",
    "test = rental_utils.one_hot_encode(dv_region, test, 'RegionID')\n",
    "\n",
    "independent = (['bathrooms', 'bedrooms', 'rooms', 'price', 'price_per_room'] + feature_names + \n",
    "    ['description_length', 'n_features', 'n_photos'] +\n",
    "    ['created_year', 'created_month', 'created_weekday', 'created_hour'] +\n",
    "    [x for x in train.columns.values if 'County' in x] +\n",
    "    [x for x in train.columns.values if 'Name' in x] +\n",
    "    [x for x in train.columns.values if 'Region' in x]\n",
    "    )\n",
    "X_train, X_val, y_train, y_val = train_test_split(train[independent], train['interest_level'], test_size=0.33, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    }
   ],
   "source": [
    "clf = xgb.XGBClassifier()\n",
    "\n",
    "param_dist = {\"learning_rate\": [0.05, 0.1, 0.2],\n",
    "              \"n_estimators\": sp_randint(10,1000),\n",
    "              \"max_depth\": sp_randint(6,10),\n",
    "              \"colsample_bytree\": [0.33, 0.5, 0.75, 1]\n",
    "             }\n",
    "\n",
    "n_iter_search = 20\n",
    "random_search = RandomizedSearchCV(clf, verbose=1,\n",
    "                                   param_distributions=param_dist,\n",
    "                                   n_iter=n_iter_search)\n",
    "random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Identify optimal hyperparameter values\n",
    "best_learning_rate     = random_search.best_params_['learning_rate']\n",
    "best_n_estimators = random_search.best_params_['n_estimators']  \n",
    "best_max_depth = random_search.best_params_['max_depth']  \n",
    "best_colsample_bytree = random_search.best_params_['colsample_bytree']  \n",
    "\n",
    "print(\"The best performing learning_rate value is:    {}\".format(best_learning_rate))\n",
    "print(\"The best performing n_estimators value is:     {}\".format(best_n_estimators))\n",
    "print(\"The best performing max_depth value is:        {}\".format(best_max_depth))\n",
    "print(\"The best performing colsample_bytree value is: {}\".format(best_colsample_bytree))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model = random_search.best_estimator_\n",
    "preds, probs = rental_utils.predict(model, X_val)\n",
    "print('Log loss:            ' + str(round(log_loss(y_val, probs), 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(model, open(\"Models/best_xgb.pkl\", \"wb\"))\n",
    "model2 = pickle.load(open(\"Models/best_xgb.pkl\", \"rb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
